\documentclass{beamer}

\usetheme{Berkeley}

\numberwithin{equation}{section} % Number equations with sections

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{makeidx}
\usepackage{tabularx}
\usepackage{url}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage{fancyvrb}
\usepackage{color}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{alltt}
% use \usepackage[pdfborder=0in]{hyperref} instead to disable red box links
%\usepackage[T1]{fontenc}

\title{Using a Neural Network to Solve the NP-Complete Subset Sum Problem}
\author{Aaron Meurer}
\date{November 17, 2011}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Agenda}
    \tableofcontents
\end{frame}

\section{Introduction to the Problem}

\begin{frame}
    \frametitle{Subset Sum Problem}
    \begin{itemize}
        \item \textbf{Problem:} Given a finite set of positive and negative integers, determine if there exists a subset that sums to zero.
        \pause
        \item Formally, given a finite set $A \subset \mathbb{Z}_+ \cup \mathbb{Z}_-$, determine if there exists $B \subseteq A$ such that $\sum_{x\in B}x = 0$.
        \pause
       \item \textbf{Example:} $A=\{1, 5, -14, -6, 3\}$ satisfies the property, with $B=\{1, 5, -6\}$ because $1 + 5 + -6 = 0$.
       \pause
       \item $A=\{2, 6, 8, -7\}$ does not satisfy the property (no subset sums to 0).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Subset Sum Problem}
    \begin{itemize}
        \item The subset sum problem is NP-Complete.
        \pause
        \item In general terms, it means that no one knows how to solve the problem efficiently.
        \pause
        \item The brute force approach requires checking all $2^n - 1$ subsets of a set of size $n$, which, due the exponential growth with respect to $n$, become unpractical for all but small $n$.
    \end{itemize}
\end{frame}

\section{Neural Network Implementation}

\begin{frame}
    \frametitle{Neural Network Implementation}
    \begin{itemize}
        \item I implemented the multilayer backpropagation neural network algorithm from scratch in Python.
        \pause
        \item This allowed me to tweak things with great precision.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Parameters}
    \begin{itemize}
        \item Start with random weights from -1 to 1.
        \pause
        \item Each layer in the network has a bias node, which does not receive input from the previous layer.
        \pause
        \item Initialize bias node weights to 1.
        \pause
        \item Train the bias nodes with the same backpropagation rule as the other nodes.
        \pause
        \item The bias nodes are essential if the test set contains the pattern $[0, 0, \ldots, 0]$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Parameters}
    \begin{itemize}
        \item $\eta_1 = 1$ (learning rate)
        \pause
        \item Update with rule:
        \begin{equation}
            \eta_{n+1} =
            \begin{cases}
               \eta_n + a & \text{if } E_n < E_{n - 1} \text{ and } E_{n - 1} < E_{n - 2} \\
               \eta_n*b & \text{if }E_n > E_{n - 1} \\
               \eta_n & \text{otherwise}
          \end{cases},
        \end{equation}
        where $E=\sum{(ti - oi)^2}/2$ is the total error.
        \pause
        \item Only increasing $\eta$ when the error decreases two times in a row is important; if we just use $E_{n - 1} < E_{n - 2}$, $E$ will oscillate.
        \pause
        \item Use $a=1$ and $b=0.01$.
        \pause
        \item Using a large $a$ leads to much faster convergence.
        \pause
        \item Use $\alpha = 0.9$ (momentum parameter)
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Architecture}
    \begin{itemize}
        \item I settled on using four layers of nodes.
        \pause
        \item The first three layers have $n$ nodes, and the last layer has one node (for binary output).
        \pause
        \item I tested for $n\in\{10, 20, \ldots, 80\}$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Pattern Set}
    \begin{itemize}
        \item Problems are already sets of numbers, so encode them as they are.
        \pause
        \item A simple script generates problems, and tests if they satisfy the property.
        \pause
        \item Generate 100 sets of 10 numbers ranging from -500 to 500.
        \pause
        \item This seemed to satisfy the property roughly 50\% of the time.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Pattern Set}
    \begin{itemize}
        \item \textbf{Example:}
        \footnotesize
        % For some reason, verbatim doesn't work here.
        \begin{alltt}
        (-337, -98, -274, 438, -135, 22, -6, 315, -188, -67), 1\\
        (-457, -344, 275, 466, 112, 391, 218, 86, -339, -332), 1\\
        (-60, 329, -4, 363, -383, 166, -313, 382, -134, -307), 0\\
        (1, 470, 156, 497, -90, -349, 267, -186, -418, 296), 1\\
        (22, -147, -260, -99, 199, -209, -131, 266, 278, 482), 1\\
        (53, 35, 360, -18, 85, 103, 447, -320, -141, 307), 0\\
        (398, -273, 382, -54, -192, 58, 79, -167, 277, 143), 1\\
        (-83, -90, -495, -304, -300, -406, -418, 365, -90, -64), 0
        \end{alltt}
        \normalsize
        \item The first item is the set, and the second is 1 if the set satisfies the property and 0 otherwise (this is the output).
        \pause
        \item Output of the network is considered a 1 if it is $\geq 0.5$ and 0 otherwise.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Convergence}
    \begin{itemize}
        \item \textbf{Convergence condition: } The network is considered to have converged if all outputs of for the training set are correct.
        \pause
        \item Give up after 200 iterations (convergence is unlikely at that point).
        \pause
        \item Do not use the total error $E$.
        \pause
        \begin{itemize}
            \item This does not seem to make a difference, and setting an $\epsilon$ too low can lead to over training.
            \pause
            \item Total error must be no larger than $\sum_{i=1}^{100}{0.5^2}/2=12.5$ for all testing data to be correct.
            \pause
            \item The network usually starts with $E$ around this number.
            \pause
            \item Usually $E < 1$ if convergence is achieved.
        \end{itemize}
        \pause
        \item Sometimes, the network does not converge, because one or two patterns is ``stuck'' on the wrong side of 0.5.
        \pause
        \item So test regardless if we reach convergence or not.
    \end{itemize}
\end{frame}

\section{Results}

\begin{frame}
    \frametitle{Results}
    \begin{itemize}
        \item \textbf{Plan:}
        \begin{itemize}
            \item Generate 100 training patterns and 100 testing patterns.
            \pause
            \item Run the network for $n\in \{10, 20, \ldots, 80\}$.
            \pause
            \item Repeat
        \end{itemize}
    \end{itemize}
\end{frame}

\section{Questions}

\begin{frame}
    \frametitle{Questions?}
    \huge{Questions?}
\end{frame}

\end{document}
