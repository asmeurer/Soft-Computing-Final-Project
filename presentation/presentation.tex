\documentclass{beamer}

\usetheme{Berkeley}

\numberwithin{equation}{section} % Number equations with sections

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{makeidx}
\usepackage{tabularx}
\usepackage{url}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage{fancyvrb}
\usepackage{color}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{alltt}
% use \usepackage[pdfborder=0in]{hyperref} instead to disable red box links
%\usepackage[T1]{fontenc}

\title{Using a Neural Network to Solve the NP-Complete Subset Sum Problem}
\author{Aaron Meurer}
\date{November 17, 2011}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Agenda}
    \tableofcontents
\end{frame}

\section{Introduction to the Problem}

\begin{frame}
    \frametitle{Subset Sum Problem}
    \begin{itemize}
        \item \textbf{Problem:} Given a finite set of positive and negative integers, determine if there exists a subset that sums to zero.
        \pause
        \item Formally, given a finite set $A \subset \mathbb{Z}_+ \cup \mathbb{Z}_-$, determine if there exists $B \subseteq A$ such that $\sum_{x\in B}x = 0$.
        \pause
       \item \textbf{Example:} $A=\{1, 5, -14, -6, 3\}$ satisfies the property, with $B=\{1, 5, -6\}$ because $1 + 5 + -6 = 0$.
       \pause
       \item $A=\{2, 6, 8, -7\}$ does not satisfy the property (no subset sums to 0).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Subset Sum Problem}
    \begin{itemize}
        \item The subset sum problem is NP-Complete.
        \pause
        \item In general terms, this means that no one knows how to solve the problem efficiently.
        \pause
        \item The brute force approach requires checking all $2^n - 1$ subsets for a set of size $n$, which, due the exponential growth with respect to $n$, makes it unpractical for all but small $n$.
    \end{itemize}
\end{frame}

\section{Neural Network Implementation}

\begin{frame}
    \frametitle{Neural Network Implementation}
    \begin{itemize}
        \item I implemented the multilayer backpropagation neural network algorithm from scratch in Python.
        \pause
        \item This allowed me to tweak things with great precision.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Parameters}
    \begin{itemize}
        \item Start with random weights from -1 to 1.
        \pause
        \item Each layer in the network has a bias node, which does not receive input from the previous layer.
        \pause
        \item Initialize bias node weights to 1.
        \pause
        \item Train the bias nodes with the same backpropagation rule as the other nodes.
        \pause
        \item The bias nodes are essential if the test set contains the pattern $[0, 0, \ldots, 0]$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Parameters}
    \begin{itemize}
        \item $\eta_1 = 1$ (learning rate)
        \pause
        \item Update with rule:
        \begin{equation}
            \eta_{n+1} =
            \begin{cases}
               \eta_n + a & \text{if } E_n < E_{n - 1} \text{ and } E_{n - 1} < E_{n - 2} \\
               \eta_n*b & \text{if }E_n > E_{n - 1} \\
               \eta_n & \text{otherwise}
          \end{cases},
        \end{equation}
        where $E_n=\sum_i{(t_i - o_i)^2}/2$ is the total error for epoch $n$.
        \pause
        \item Only increasing $\eta$ when the error decreases two times in a row is important; if we just use $E_{n - 1} < E_{n - 2}$, $E$ will oscillate.
        \pause
        \item Use $a=1$ and $b=0.01$.
        \pause
        \item Using a large $a$ leads to much faster convergence.
        \pause
        \item Use $\alpha = 0.9$ (momentum parameter)
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Architecture}
    \begin{itemize}
        \item I settled on using four layers of nodes.
        \pause
        \item The first three layers have $n$ nodes, and the last layer has one node (for binary output).
        \pause
        \item I tested for $n\in\{10, 20, \ldots, 80\}$.
        \pause
        \item The activation for each node was the sigmoid (actually, a scaled hyperbolic tangent) function with $k=\frac{1}{20}$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Pattern Set}
    \begin{itemize}
        \item Input patterns are already sets of numbers, so encode them as they are.
        \pause
        \item A simple script generates problems, and tests if they satisfy the property.
        \pause
        \item Generate 100 sets of 10 integer numbers ranging from -500 to 500 (not including 0).
        \pause
        \item This seemed to satisfy the property roughly 50\% of the time.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Pattern Set}
    \begin{itemize}
        \item \textbf{Example:}
        \footnotesize
        % For some reason, verbatim doesn't work here.
        \begin{alltt}
        (-337, -98, -274, 438, -135, 22, -6, 315, -188, -67), 1\\
        (-457, -344, 275, 466, 112, 391, 218, 86, -339, -332), 1\\
        (-60, 329, -4, 363, -383, 166, -313, 382, -134, -307), 0\\
        (1, 470, 156, 497, -90, -349, 267, -186, -418, 296), 1\\
        (22, -147, -260, -99, 199, -209, -131, 266, 278, 482), 1\\
        (53, 35, 360, -18, 85, 103, 447, -320, -141, 307), 0\\
        (398, -273, 382, -54, -192, 58, 79, -167, 277, 143), 1\\
        (-83, -90, -495, -304, -300, -406, -418, 365, -90, -64), 0
        \end{alltt}
        \normalsize
        \pause
        \item The first item is the set.
        \item The second is 1 if the set satisfies the property and 0 otherwise (this is the output).
        \pause
        \item Output of the network is considered a 1 if it is $\geq 0.5$ and 0 otherwise.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Convergence}
    \begin{itemize}
        \item \textbf{Convergence condition: } The network is considered to have converged if all outputs for the training set are correct.
        \pause
        \item Give up after 200 iterations (convergence is unlikely at that point).
        \pause
        \item \textit{Do not} use the total error $E$.
        \pause
        \begin{itemize}
            \item This does not seem to make a difference, and setting an $\epsilon$ too low can lead to over training.
            \pause
            \item Total error must be no larger than $\sum_{i=1}^{100}{0.5^2}/2=12.5$ for all testing data to be correct.
            \pause
            \item The network usually starts with $E$ around this number with random weights.
            \pause
            \item Usually $E < 1$ if convergence is achieved.
        \end{itemize}
        \pause
        \item Sometimes, the network does not converge, because one or two patterns is ``stuck'' on the wrong side of 0.5.
        \pause
        \item So test regardless if we reach convergence or not.
    \end{itemize}
\end{frame}

\section{Results}

\begin{frame}
    \frametitle{Results}
    \textbf{Plan:}
    \begin{itemize}
        \item Generate 100 training patterns and 100 testing patterns.
        \pause
        \item Run the network for $n\in \{10, 20, \ldots, 80\}$.
        \pause
        \item Repeat a total of 21 times (I will do 50 for the report)
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Results}
    \textbf{The big question: does it work?}
    \uncover<4->{\textbf{Answer: No}}
    \normalsize
    \uncover<1-2>{
    \begin{itemize}
        \item Actual average output of the test data:\\
        $0.46 \pm 0.088$ (two standard deviations), i.e., $[0.372, 0.548]$.
        \pause
        \item Output of the network:\\
            \begin{tabular}{|c|c|c|c|}
                \hline
                Node & Mean & Mean accuracy \\
                architecture & accuracy & $\pm 2*\text{standard deviation}$ \\
                \hline
                (10, 10, 10, 1) & 0.4948 & [0.3844, 0.6051]\\
                \hline
                (20, 20, 20, 1) & 0.5167  & [0.4158, 0.6176]\\
                \hline
                (30, 30, 30, 1) & 0.4952  & [0.4091, 0.5813]\\
                \hline
                (40, 40, 40, 1) & 0.5105  & [0.4128, 0.6082]\\
                \hline
                (50, 50, 50, 1) & 0.5014  & [0.4090, 0.5939]\\
                \hline
                (60, 60, 60, 1) & 0.5210  & [0.4341, 0.6078]\\
                \hline
                (70, 70, 70, 1) & 0.5148  & [0.4171, 0.6125]\\
                \hline
                (80, 80, 80, 1) & 0.5090  & [0.4242, 0.5939]\\
                \hline
            \end{tabular}
    \end{itemize}
    }
\end{frame}

\begin{frame}
    \frametitle{Results}
    \begin{itemize}
        \item Increasing the number of nodes doesn't even in increase the accuracy.
        \pause
        \item All it does is increase the rate of convergence:\\
        \footnotesize
        \begin{tabular}{|c|c|c|}
            \hline
            Node & \% of training patterns & Average number \\
            architecture &  that converged & of epochs to converge\\
             & in $< 200$ epochs &  for such patterns\\
            \hline
            (10, 10, 10, 1) & 0.0 & --\\
            \hline
            (20, 20, 20, 1) & 0.14 & 152.0\\
            \hline
            (30, 30, 30, 1) & 0.33 & 105.7\\
            \hline
            (40, 40, 40, 1) & 0.19 & 119.5\\
            \hline
            (50, 50, 50, 1) & 0.38 & 109.6\\
            \hline
            (60, 60, 60, 1) & 0.57 & 71.3\\
            \hline
            (70, 70, 70, 1) & 0.71 & 76.7\\
            \hline
            (80, 80, 80, 1) & 0.76 & 56.0\\
            \hline
        \end{tabular}
        \normalsize
    \end{itemize}
\end{frame}

\section{Conclusions}

\begin{frame}
    \frametitle{Conclusions}
    \begin{itemize}
        \item I wasn't really expecting this to work (NP-Complete problems are very hard).
        \pause
        \item Even if it did work, it would be very inefficient (on the other hand, computing all the subsets for sets of size 10 is instantaneous).
        \pause
        \item And it also wouldn't generalize to sets with more than $n$ elements.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Other possible things to look at}
    \begin{itemize}
        \item Given that a set satisfies the property, can a network find the subset?
        \pause
        \begin{itemize}
            \item Make a network with $n$ output nodes and train the network to return a 1 in each node corresponding to elements of the set to pick.
            \pause
            \item For example, [1, 5, -14, -6, 3] would return [1, 1, 0, 1, 0].
        \end{itemize}
        \pause
        \item Compute errors based on how ``close'' the network was.
        \item In other words, use the floating point output of the network, rather than rounding it to 0 or 1.
        \pause
        \item Find some measure of how ``close'' a set that doesn't satisfy the property is to satisfying it, and compare that to the error of the set.
        \begin{itemize}
            \item e.g., $\{2, 4\}$ is not close, but $\{-1, 2\}$ is.
        \end{itemize}
    \end{itemize}
\end{frame}

\section{Questions}

\begin{frame}
    \frametitle{Questions?}
    \huge{Questions?}
\end{frame}

\end{document}
